---
title: |
  |
  |
  | A Practical Review of Multiple Linear Regression
  |
  |
author: |
  | Advanced Topics in Quant Social Research
  |
output:
  beamer_presentation:
    includes:
      in_header: mystyle.tex
  slidy_presentation: default
  ioslides_presentation: default
fontsize: 10pt
---

# Plan for the day

\begin{itemize}
	\item Recap: linear regression model
	\vspace{0.1cm}
	\begin{itemize}
		\item Definition and setup
		\item Data analytical objectives
		\item Assumptions and inference
	\end{itemize}
	\vspace{0.3cm}
	\item "Multiple" linear regression
	\vspace{0.1cm}
	\begin{itemize}
		\item Why "multiple"
		\item Principles of model specification
		\item Practical reminders
	\end{itemize}
	\vspace{0.3cm}
	\item Looking ahead: "All models are wrong, but some are useful" (Box and Draper, 1987)
\end{itemize}

# What is linear regression model

\begin{itemize}
	\item Statistical inference aims to understand the \textbf{relationship} between different \textbf{variables}.
	\vspace{0.3cm}
	\item The linear regression model is a common technique to use a \textbf{linear function} to represent and \textbf{estimate} the statistical relationship between $\textbf{X}$ and $\textbf{Y}$, using the data we have.
	\vspace{0.5cm}
	\begin{center}
$\textbf{Y}=\underbrace{\alpha}_{\text{intercept}}+\underbrace{\beta}_{\text{slope}}\textbf{X}+\underbrace{\epsilon}_{\text{error or disturbance}},$
	\end{center}
	where
	\vspace{0.1cm}
	\begin{itemize}
		\item $\textbf{Y}$ is the outcome or response variable
		\item $\textbf{X}$ is the predictor or independent variable
	\end{itemize}
\end{itemize}

# What is linear regression model

\vspace{0.5cm}
\begin{center}
$\textbf{Y}=\underbrace{\alpha}_{\text{intercept}}+\underbrace{\beta}_{\text{slope}}\textbf{X}+\underbrace{\epsilon}_{\text{error}},$
\end{center}
where
\vspace{0.2cm}
\begin{itemize}
	\item \textbf{Intercept}: The corresponding value of $\textbf{Y}$ when $\textbf{X}$ is set at 0.
	\vspace{0.2cm}
	\item \textbf{Slope}: The corresponding change in the value of $\textbf{Y}$ when we increase $\textbf{X}$ by one unit.
	\vspace{0.2cm}
	\item \textbf{Error} or \textbf{disturbance}: The corresponding deviation in the value of $\textbf{Y}$ from the predicted $\widehat{\textbf{Y}}$ as follows:
	\vspace{0.1cm}
	\begin{equation}
	\begin{split}
	\epsilon&=\text{actual outcome}-\text{predicted outcome}\\
	&=\textbf{Y}-\widehat{\textbf{Y}}\\
	&=\textbf{Y}-(\alpha+\beta\textbf{X})
	\end{split}
	\end{equation}
\end{itemize}

---

\vspace{0.3cm}
```{r echo=FALSE, out.width="87%", fig.align="center"}
knitr::include_graphics("Figs/tufte.pdf")
```

---

\vspace{0.3cm}
```{r echo=FALSE, out.width="87%", fig.align="center"}
knitr::include_graphics("Figs/imai.pdf")
```

# Different views of linear regression model

\begin{itemize}
	\item We use the linear regression model to estimate or quantify the statistical relationship, or \textbf{correlation}, between $\textbf{X}$ and $\textbf{Y}$.
	\vspace{0.3cm}
	\item Depending on different data analytical objectives, $\textbf{X}$ can be understood in different ways (BdM and Fowler 2021).
	\vspace{0.1cm}
	\begin{itemize}
		\item Regression for \textbf{explanation} is to use $\textbf{X}$ (as explanatory variable) to explain the variation in $\textbf{Y}$.
		\item Regression for \textbf{forecasting} is to use $\textbf{X}$ (as predictor) to predict $\textbf{Y}$.
	\end{itemize}
	\vspace{0.3cm}
	\item Regression for \textbf{causal inference}: With certain assumptions (more after Week 5), the estimated slope (or $\widehat{\beta}$) can be considered as the \textbf{marginal effect} of $\textbf{X}$ (as cause or treatment).
\end{itemize}

# Statistical inference of linear regression

\begin{itemize}
	\item For the linear regression model to produce \textbf{unbiased} $\widehat{\beta}$ (i.e., the estimated slope is the true slope), we need several assumptions or conditions (Roback and Legler 2020).
	\vspace{0.1cm}
	\begin{itemize}
		\item Linearity: There is a \textbf{linear} relationship between $\textbf{X}$ and $\textbf{Y}$.
		\item Independence: The errors of individual observations (i.e., $\textbf{Y}-\widehat{\textbf{Y}}$) are \textbf{independent} of each other.
		\item Normality: Across different values/levels of $\textbf{X}$, $\textbf{Y}$ is \textbf{normally distributed}.
		\item Homoscedasticity: Across different values/levels of $\textbf{X}$, the variance or standard error of $\textbf{Y}$ is {equal}
	\end{itemize}
	\vspace{0.3cm}
	\item \textbf{[Note]} These assumptions do not say the linear regression model will produce \textbf{efficient} $\widehat{\beta}$ (i.e., the estimated slope may still have large variance or standard error).
\end{itemize}

# Statistical inference of linear regression

We can combine the \textbf{independence}, \textbf{normality} and \textbf{homoscedasticity} assumptions and re-state them: \textbf{Across different values/levels of $\textbf{X}$, $\textbf{Y}$ should be independent and identically (and normally) distributed}.
\vspace{0.1cm}
```{r echo=FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("Figs/assumptions.pdf")
```

# Statistical inference of linear regression

\begin{itemize}
	\item Many statistical tools have been developed to detect and evaluate assumption violations.
	\vspace{0.3cm}
	\item The violation of these assumptions is by no means a deal break, as many statistical tools or alternative models/regression estimators have been developed to address these situations.
	\vspace{0.1cm}
	\begin{itemize}
		\item \textbf{Generalized linear models} are developed to get round the linearity and normality assumptions (more in Weeks 3-4).
		\item \textbf{Time-series analysis} is a well-known technique to tackle the violation of the independence assumption (not covered).
		\item \textbf{Robust} or \textbf{clustered} standard errors are developed to address the violation of the homoscedasticity assumption (not covered).
		\item It is also possible to ditch the linear regression and, instead, use \textbf{non-parametric} or \textbf{Bayesian} statistical analysis (more in Week 11).
	\end{itemize}
\end{itemize}

# Statistical inference of linear regression

\begin{itemize}
	\item Is $\beta$ "statistically" significant? When using the linear model to estimate $\beta$, we we need to evaluate two hypotheses:
	\begin{equation}
	\begin{split}
	& H_0: \beta=0\\
	& H_1: \beta\neq 0
	\end{split}
	\end{equation}
	\item The \textbf{null} hypotheses ($H_0$) means $\textbf{X}$ and $\textbf{Y}$ are not correlated while the \textbf{alternative} hypotheses ($H_1$) says the opposite.
	\item To show correlation, we need to reject $H_0$.
	\begin{itemize}
		\item The $p$-value: The conditional probability of observing our data given $H_0$ (the probability should be low enough for us to reject $H_0$)
		\item The confidence interval: The possible range of our $\widehat{\beta}$ (the range should not include 0 for us to reject $H_0$)
	\end{itemize}
	\item In addition to \textbf{statistical} significance, we should also check the sign and size of $\widehat{\beta}$.
\end{itemize}

# Statistical inference of linear regression

\begin{itemize}
	\item Is the proposed model specification the best \textbf{fit}? That is, does the model provide $\widehat{\textbf{Y}}$ with smallest residuals?
	\vspace{0.3cm}
	\begin{itemize}
		\item $R^2$: The idea is to \textbf{maximize} the proportion of variance of $\textbf{Y}$ explained by the variance of $\widehat{\textbf{Y}}$.
		\vspace{0.1cm}
		\begin{equation}
		R^2=\frac{\text{variance of }\widehat{\textbf{Y}}}{\text{variance of }\textbf{Y}}
		\end{equation}
		\vspace{0.1cm}
		\item Sum of squared residuals (SSR): The idea of \textbf{least squares} is to \textbf{minimize} the SSR.
		\vspace{0.1cm}
		\begin{equation}
		\text{\textbf{SSR}} = \sum_{i=1}^n \widehat{\epsilon}_i^2=\underbrace{\widehat{\epsilon}_1^2+\widehat{\epsilon}_2^2+\widehat{\epsilon}_3^2+\widehat{\epsilon}_4^2+\widehat{\epsilon}_5^2\dots+\widehat{\epsilon}_{n-1}^2+\widehat{\epsilon}_n^2}_{\text{sum of the squared estimated errors of $n$ observations}},
		\end{equation}
		where $n$ refers to the number of observations in the model.
	\end{itemize}
\end{itemize}

# "Multiple" linear regression

\begin{itemize}
	\item Given the complexity of the social world, it is impossible to use a single \textbf{X} to predict or explain the outcome of interest.
	\vspace{1cm}
	\item The \textbf{Multiple} (or \textbf{multivariate}) linear model is the most widely used linear regression model where we include more than one $\textbf{X}$.
	\vspace{1cm}
	\item If we use $k$ to refer to the number of $\textbf{X}$s in the model, a multiple linear regression model can be represented as follows
	\vspace{0.1cm}
	\begin{equation}
	\textbf{Y}=\alpha+\beta_1\textbf{X}_1+\beta_2\textbf{X}_2+\dots+\beta_k\textbf{X}_k+\epsilon,
	\end{equation}
	where $k$ refers to the $k$th $\textbf{X}$.
\end{itemize}

# "Multiple" linear regression	

\vspace{0.3cm}
\begin{center}
$\textbf{Y}=\alpha+\widehat{\beta}_1\textbf{X}_1+\dots+\widehat{\beta}_k\textbf{X}_k$
\end{center}
\vspace{0.5cm}
\begin{itemize}	
	\item In the model above, $\widehat{\beta}_i$ refers to the slope of a particular $X_i$.
	\vspace{0.3cm}
	\item The estimated slope of $X_i$, $\widehat{\beta}_i$, can describe the correlation between $\textbf{Y}$ and $\textbf{X}_i$ when we \textbf{control for} other predictors in the model.
	\vspace{0.3cm}
	\item Likewise, there can be different interpretations:
	\begin{itemize}
		\item $\beta_i$ shows the correlation between $\textbf{X}_i$ and $\textbf{Y}$ while accounting for the association between other $\textbf{X}$s and $\textbf{Y}$.
		\item $\beta_i$ shows the causal effect of $\textbf{X}_i$ on $\textbf{Y}$ while ruling out the effect of other $\textbf{X}$s on $\textbf{Y}$ (with certain assumptions).
	\end{itemize}
\end{itemize}

# "Multiple" linear regression

\begin{itemize}
	\item When we include multiple $\textbf{X}$s in our model,
	\begin{itemize}
		\item We are less likely to commit the \textbf{omitted variable bias}, but
		\item We need to check for \textbf{collinearity} (using \textbf{variance inflation factor}) and \textbf{overfitting} (using \textbf{adjusted} $R^2$).
	\end{itemize}
	\vspace{0.5cm}
	\item We can also consider more complicated statistical associations between $\textbf{Y}$ and different $\textbf{X}$ by including
	\begin{itemize}
		\item Interaction term (e.g., $\textbf{Y}=\alpha+\beta_1\textbf{X}_1+\beta_2\textbf{X}_2+\beta_3\textbf{X}_1\textbf{X}_2+\epsilon$)
		\item Quadratic or polynomial terms (e.g., $\textbf{Y}=\alpha+\beta_1\textbf{X}_1+\beta_2\textbf{X}_1^2+\epsilon$)
	\end{itemize}
	\vspace{0.5cm}
	\item Model comparison is recommended but not essential.
\end{itemize}

---

\vspace{0.3cm}
```{r echo=FALSE, out.width="87%", fig.align="center"}
knitr::include_graphics("Figs/quadric.pdf")
```
\begin{equation}
\textbf{Y}=1+2\textbf{X}+0.5\textbf{X}^2
\end{equation}

# Conclusion: "All models are wrong, but some are useful"

\begin{itemize}
	\item Practical reminders to build your multiple linear regression model,
	\vspace{0.1cm}
	\begin{itemize}
		\item Drawing on your knowledge of the subject matter, \textbf{specify a baseline model} including all key explanatory variables to avoid \textbf{omitted} variable bias.
		\item Check if the baseline model falls into the victim of \textbf{collinearity} and/or \textbf{overfitting}.
		\item Consider more complicated model specification techniques, such as interaction and/or quadratic/polynomial terms.
		\item Consider alternative estimators or modeling choices, such as generalized linear model (Week 3) and/or multilevel/hierarchical modeling (Week 4)
	\end{itemize}
	\vspace{0.3cm}
	\item \textbf{All models are approximations}. Good analysis requires careful decision-making while holding a solid knowledge of the subject matter. 
\end{itemize}
 
# Key texts
 
\begin{itemize}
	\item \textit{Quantitative Social Science: An Introduction} (Imai 2018), Chapter 4
	\vspace{0.5cm}
	\item \textit{Beyond Multiple Linear Regression} (Roback and Legler 2020), Chapter 1
	\vspace{0.5cm}
	\item \textit{Thinking Clearly With Data} (BdM and Fowler 2022), Chapter 2
	\vspace{0.5cm}
	\item \textit{The Effect} (Huntington-Klein 2022), Chapters 4 and 13
\end{itemize}

# Example: Fitting OLS to 4 Observations

\vspace{0.3cm}
```{r echo=FALSE, out.width="87%", fig.align="center"}
knitr::include_graphics("Figs/example_new.pdf")
```


