---
title: "Tutorial: Analyzing Survey Data (Part 2)"
author: ""
date: ""
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

Chao-Yo Cheng\
6 February 2021

# Before You Start

Revisit **Tutorial: logistic regression as a GLM** prepared by Dr Marju Kaps -- let's have a quick discussion on this.

**If you'd like, please focus on Section 12 only and treat other sections in the original tutorial as extra materials**. 

Section 7 in the original tutorial is useful if you need a quick refresher on the log function, but that will not be included in the exam.

# 1 Introduction

By the end of this tutorial, you will know how to:

  - Review the main concepts of logit regression and how to interpret the results.
  - Use logit regression to analyze survey data.
  - Perform model comparison on survey data.

We are building on previous tutorials on logit regression and survey weights this week. You can continue writing your code in the same file you used last week. We will start by bringing in the same packages again.

```{r, echo=T, eval=T, message=F}
library(survey)
library(srvyr)
library(dplyr)
library(ggplot2)
library(purrr)
```

# 2 Data

We will continue to work with the data set from **the 2011 Canadian National Election Study**, which includes the following variables.

  - `id` -- a unique identifier for each response.
  - `province` -- a factor with (alphabetical) levels, including AB, BC, MB, NB, NL, NS, ON, PE, QC, SK (each of these refers to a Canadian province). The sample was "stratified" by province.
  - `population` -- population of the respondent's province (number of people over age 17).
  - `weight` -- weight sample to size of population, taking into account unequal sampling probabilities by province and household size.
  - `abortion` -- attitude toward abortion, a factor with levels `No` and `Yes`; answer to the question "Should abortion be banned?"
  - `gender` -- a factor with two levels `Female` and `Male`.
  - `importance` -- importance of religion, a factor with (alphabetical) levels including `not`, `notvery`, `somewhat`, `very`; answer to the question, "In your life, would you say that religion is very important, somewhat important, not very important, or not important at all?"
  - `education` -- a factor with (alphabetical) levels including `bachelors` (Bachelors degree), `college` (community college or technical school), `higher` (graduate degree), HS (high-school graduate), `lessHS` (less than high-school graduate), `somePS` (some post-secondary).
  - `urban` -- place of residence, a factor with levels `rural`, `urban`.

In this tutorial, we will consider some of the variables that may be statistically associated with people's attitudes towards abortion. The outcome of variable of interest is therefore `abortion`.

> *Question: Anything wrong with the current set of explanatory/independent variables or predictors? Any other factors we should consider?*

Let's read in the data and check that all of our variables are as described.

```{r, echo=T, eval=F, message=F}
ces <- read.csv("ces11.csv", stringsAsFactors = TRUE)
head(ces,10)
```

```{r, echo=F, eval=T}
ces <- carData::CES11
head(ces,10)
```

# 3 Use `glm()` to Run Logit Regression

*Note: The main objective of this section is to make sure everyone is on the same page with respect to logit and GLM before we move on to include weights in the analysis.*

In this section, we will use the workhorse function `glm()` to estimate the correlates of *people's attitudes toward abortion*.

  - Create a new dependent variable `against_abortion`, using the original variable `abortion` in the dataset.
  
  - Fit the logit model with no predictor. Interpret the findings.
  
  - Fit the logit model with one predictor. Interpret the findings using odds-ratio with the help of `predict()`.

First, let's create a binary variable such that we will assign the value of 1 to people against abortion in the study. If we use the `tidyverse` style, then we will use the `mutate()` function after the first pipeline. Make sure you save the data frame with the new variable as the same or another object, so we can continue to use it in the following analysis.

```{r, echo=T, eval=T}
ces_new <- ces %>%
  mutate(against_abortion = if_else(abortion == "No", 1, 0))
```

The `if_else()` function is very straightforward -- it basically says: *please assign the value of 1 to those who answered "No" and 0 to those who answered the opposite.*

Now let's check if we have done this properly. First, use `table()`.

```{r, echo=T, eval=T}
table(ces_new$against_abortion)
```

All observations are placed properly (i.e., no respondent is mis-classified in the new variable).

> *Question: Can you create a new binary variable such that it assigns the value of 1 (and 0 otherwise) to those living in the urban area?*

## 3.1 Logit Regression with No Predictor

We will tart with a simple model with no predictor.

Quickly recall the logit regression transforms a **probability** into **log odds** (or logged odds or log of odds, they are interchangeable): $$\text{logit}(p)=\log\left(\frac{p}{1-p}\right).$$ In this case, $p$ is the probability of respondents saying "No" to abortion. 

If we only include the intercept (i.e., no predictor), then our model looks like this: $$\log\left(\frac{p}{1-p}\right)=\alpha.$$ The estimated intercept will be *the log-odds of people are against abortion*.

Let's go ahead and fit the model. Use the `summary()` function to see the canonical regression output. Note that we have to use the new data frame `ces_new` (the original `ces` does not include the variable we created).

```{r, echo=T, eval=T}
mod_intercept <- glm(against_abortion ~ 1,
                     data = ces_new,
                     family = binomial)
summary(mod_intercept)
```

> *Question: Use `?glm` to see more information -- what is the default for the option `family`?*

### 3.1.1 Explore the Point Estimate

The estimated intercept $\hat{\alpha}=1.482045$ is the *log odds*. Let's use `coef()` to extract the coefficient.
  
```{r, echo=T, eval=T}
coefs <- coef(mod_intercept) # extracting log odds
coefs
```  
  
Next, we can exponentiate the log odds to get the *odds* (i.e., use `exp()` to get rid of the log from the odds).
  
```{r, echo=T, eval=T}
exp(coefs) # calculating odds
```

Third, let's take an extra step to compute the *probability* of people saying no to abortion. From the previous step, since the odds $\frac{p}{1-p}=4.401937$, we know $p=\frac{4.401937}{1+4.401937}=0.8148812$.

```{r, echo=T, eval=T}
exp(coefs)/(1+exp(coefs)) # calculating p
```

### 3.1.2 Explore the Confidence Intervals of the Point Estimate

If you'd like, we can show the confidence interval of the point estimate, using the `confint()` function. These are the upper and lower bounds of the 95\% confidence intervals for $\hat{\alpha}$, namely the log odds.

```{r, echo=T, eval=T, message=F}
coefs_ci <- confint(mod_intercept)
coefs_ci
``` 

> *Question: How do you derive the upper and lower bounds of estimated odds and probability? Hint: Use the upper and lower bounds of log odds to calculate the upper and lower bounds of estimated odds and probabilities.*

## 3.2 Logit Regression with One Predictor

Now, we will add a predictor to the logit regression model.

There is no variable like this, so let's create a new variable `religion`, using `importance`. Let's use `recode()` inside `mutate()`. Here we use higher numbers to refer to greater importance.

```{r, echo=T, eval=T}
ces_new <- ces_new %>% # update "ces_new" to include all new variables
  mutate(religion = recode(importance,
                           "very" = 4,
                           "somewhat" = 3,
                           "notvery" = 2,
                           "not" = 1))
table(ces_new$importance, ces_new$religion)
```

> *Question: Which of the two versions of the importance of religion variable would you use in your analysis? Why? Try to think of pros and cons for each.*

With the predictor included, the log odds of someone saying no to abortion is then $$\log\left(\frac{p}{1-p}\right)=\alpha + \beta(\text{religion}).$$ The model means that we specify "the log-odds of people opposition abortion is a (linear) function of the variable `religion`s".

Let's run the analysis, using the new data frame.

```{r, echo=T, eval=T}
mod_religion <- glm(against_abortion ~ religion, data = ces_new, family = binomial)
summary(mod_religion)
```

The estimated coefficient for *Religion* is 1.17470 and statistically significant. How do we interpret the results? Dr Kaps introduced several options, but two stand out.

### 3.2.1 Exponentiate $\hat{\beta}$ to Get Odds Ratio (OR)

First, $e^\beta$ (i.e., enter the coefficients in the `exp()` function) will give us the **odds-ratio** (OR). OR is notoriously confusing, but here is the basic intuition with minimum math involved.

  - First, recall that we define the model as $\log\left(\frac{p}{1-p}\right)=\alpha + \beta(\text{religion}).$ The outcome variable is the log-odds of people against abortion.

  - Again, similar to OLS, $\beta$ tells us how the *log-odds* will change when we increase the importance of `religion` by one level. Say we move `religion` from 1 to 2, we will have two separate log-odds. One is the log-odds of people opposing abortion when `Religion`$=2$ and the other one is the log-odds of people opposing abortion when `Religion`$=1$. Let's name them $\log(\text{Odds when Religion}=0)$ and $\log(\text{Odds when Religion}=1)$. This gives us $$\beta=\log(\text{Odds when Religion}=2)-\log(\text{Odds when Religion}=1).$$
  
  - By the law of logarithms, the **the difference between two logs** (the left hand side below) can be rewritten as **the log of them dividing up each other** (the right hand side below): $$\log(\text{Odds when Religion}=2)-\log(\text{Odds when Religion}=1)=\log\left(\frac{\text{Odds when Religion}=2}{\text{Odds when Religion}=1}\right).$$
  
  - We can remove the log from $\log\left(\frac{\text{Odds when Religion}=2}{\text{Odds when Religion}=1}\right)$ by taking $e^\beta$; in doing so, we have $\left(\frac{\text{Odds when Religion}=2}{\text{Odds when Religion}=1}\right)$, and this is **the odds ratio when we increase the importance of religion by one unit.** 
  
```{r, echo=T, eval=T}
exp(coef(mod_religion)) # exponentiate coefficients
```  

Essentially, odds-ratio (OR) is a ratio of two different odds when we change the value of `religion` from 1 to 2. 

```{r, echo=F, eval=T, message=F}
library(readxl)
library(kableExtra)
or <- read_xlsx("or.xlsx", col_names=T)
colnames(or)[1] = ""
or %>%
  kbl() %>%
  kable_styling()
```

Since we find $OR=0.3089115$, it seems our data support the last scenario. 

To sum up our discussion, when you find a statistically significant coefficient, do the following:

  - Step 1: Put the coefficients into `exp()` to exponentiate them. This is the odds-ratio when we change the corresponding predictor by one unit. In our case, $\beta$ is the coefficient for the variable `religion`, so doing exponentiating it will give us the odds ratio when we increase the level of `religion` by 1.
  
  - Step 2: Read the odds-ratio (OR).
    - If $OR=1$, the predictor is not associated with the outcome (this is going to be rare in practice).
    - If $OR>1$, the predictor is positively associated with the outcome.
    - If $OR<1$, the predictor is negatively associated with the outcome.

### 3.2.2 Use `predict()` Function (Recommended)

```{r, echo=T, eval=T}
mod_religion <- glm(against_abortion ~ religion, data = ces_new, family = binomial)
```

Usually, I do not recommend too many calculations.

The most intuitive way to me, is to **use the `predict()` function**. Plug the logit model into `predict()` to compute the predicted **log-odds** for each observation (given each respondent's report level of religious importance). Including `type="response"` will return the predicted **probabilities**. 

```{r, echo=T, eval=T, message=F}
fit_log_odds <- predict(mod_religion) # return predicted log-odds
fit_prob <- predict(mod_religion, type="response") # return predicted probabilities
```  

Let's create a new data frame to include `religion` so you will see it better. We will also include the original `importance` variable. We also use `exp()` to remove log from log-odds.

```{r, echo=T, eval=T}
fit_religion <- data.frame(religion = ces_new$religion, # use "religion" in "ces_new"
                           importance = ces_new$importance)
fit_religion <- fit_religion %>%
  mutate(prob = predict(mod_religion, type="response"),
         log_odds = predict(mod_religion)) %>%
  mutate(odds = exp(log_odds))
head(fit_religion, 10)
```  

You will see that observations who have the identical level of importance towards religion have the same predicted log-odds and probabilities. Let's clean the data frame a bit with the help with `distinct()` and `arrange()`. 

```{r, echo=T, eval=T}
fit_religion <- fit_religion %>%
  distinct() %>% # remove duplicates
  arrange(religion) # arrange observations by religion
fit_religion
```  

Now we have the corresponding predicted odds, log-odds, and probability for each level of `religion`. If we increase the importance of `religion` from 1 to 2, we will get (you can ignore the number 8 on the top):

```{r, echo=T, eval=T}
### The corresponding change in log-odds is
beta <- fit_religion$log_odds[fit_religion$religion == 2] - fit_religion$log_odds[fit_religion$religion == 1]
beta

### The corresponding odds ratio is
or <- fit_religion$odds[fit_religion$religion == 2]/fit_religion$odds[fit_religion$religion == 1]
or
```  

They are identical with the results we get before. All in all, we know that a more religious person is less likely to go against abortion. You can also calculate the change in the probability as below.

```{r, echo=T, eval=T}
### The corresponding change in probability is
prob <- fit_religion$prob[fit_religion$religion == 2] - fit_religion$prob[fit_religion$religion == 1]
prob
```

> *Question: Which interpretation do you prefer?*\

> *Question: Redo the entire exercise, but now create a new binary variable `urban_num` so that it takes the value of 1 for those living in the city (otherwise 0) and carry out another logit regression with `urban_num` as the only predictor.*

To use `predict()` when the model includes more than one predictor, see *Section 4.4*.

# 4 Fitting GLMs With Weights

The `survey` package makes this very easy. The function `svyglm()` is identical to the conventional `glm()` function except that `svyglm()` use `design` (i.e., the survey object) instead of `data`.

Type `?svyglm` for more information about the function. Alternatively, you can visit the `survey` package's vignette page. The section **Regression models** provides additional information.

In this section, we will use the updated data frame `ces_new`, which should have included all the variables we have added so far, to carry out the following activities.

  - Create a survey object. 
  
  - Use the `svyglm()` function to fit logit regression with two predictors.
  
  - Compare the results.

## 4.1 Create a Survey Object

```{r, echo=T, eval=F}
ces_new <- ces %>%
  mutate(against_abortion = if_else(abortion == "No", 1, 0),
         urban = if_else(urban == "urban", 1, 0),
         religion = recode(importance,
                           "very" = 4,
                           "somewhat" = 3,
                           "notvery" = 2,
                           "not" = 1))
```

As before, `as_survey()` to create the survey object so we can use the useful functions in the packages.

```{r, echo=T, eval=F}
ces_s <- ces_new %>%
  as_survey(ids = id,
            strata = province,
            fpc = population,
            weights = weight)
```

## 4.2 Fit Logit Regression

To carry out logit regression with weights included, we need to use the `svyglm()` function in the `survey` package. Let's include `religion` again.

```{r, echo=T, eval=F, message=F}
mod_s_religion <- svyglm(against_abortion ~ religion,
                design = ces_s, # be sure to use the survey object
                family = binomial)
summary(mod_s_religion)
```

## 4.3 Model Comparison and Diagnostics

**Note: Please revisit Section 12 "Diagnostics" in "Tutorial: logistic regression as a GLM" by Dr Marju Kaps.** 

Let's conduct another logit regression but this time only includes the intercept.

```{r, echo=T, eval=F}
mod_s_intercept <- svyglm(against_abortion ~ 1,
                          design = ces_s,
                          family = binomial)
summary(mod_s_intercept)
```

Use `anova()` to see whether or not including `religion` helps with explaining more variance in the outcome variable (or: does including `religion` statistically improve the model fit)?

```{r, echo=T, eval=F}
anova(mod_s_intercept, mod_s_religion, test="Chi")
```

> *Question: What is the `test` option here for? Use `?anova` to see more information.*\

> *Question: You can also compare the finding we get from `glm()`.*\

## 4.4 Extra: Get Predicted Log Odds and Probability With Multiple Predictors

Say now if we include two predictors in the model.

```{r, echo=T, eval=F, message=F}
mod_s <- svyglm(against_abortion ~ religion,
                design = ces_s, # be sure to use the survey object
                family = binomial)
summary(mod_s)
```

We can still use the `predict()` function to interpret the results. Given that the model includes two predictors, we have to let `predict()` know the values each variable should take to carry out the predictions. 

In multivariate regression, we usually have to hold the other variable *constant* so as to interpret individual coefficients. The following exercise will use `religion` as the example. Against, the coefficient for `religion` tells us how the log-odds will change if we increase religion's importance by 1 when we control for the effect of `urban`.

Let's feed the `predict()` function two pieces of information:

  - What are the unique levels of `religion``? The variable ranges from 1 to 4.
  - What is the constant of `urban` we can use? Weighted mean is a good choice.

```{r, echo=T, eval=F, message=F}
ces_s %>%
  summarise(urban_w_mean = survey_mean(urban, na.rm=T))
```

Putting them together, let's create a new data frame. Note the variable names have to be the same as those included in `svyglm()`. 

```{r, echo=T, eval=F, message=F}
data_predict <- data.frame(religion = 1:4,
                           urban = 0.785)
data_predict
```

Now let's obtain the predicted log-odds, odds, and probabilities when we vary the importance of `religion` while keeping `urban` at its weighted mean. 

```{r, echo=T, eval=F, message=F}
fit_prob <- predict(mod_s, newdata=data_predict, type="response")
fit_log_odds <- predict(mod_s, newdata=data_predict)
fit_mod_s <- data.frame(religion = 1:4,
                        fit_prob = as.matrix(fit_prob),
                        fit_log_odds = as.matrix(fit_log_odds))
fit_mod_s$fit_odds <- exp(fit_mod_s$fit_log_odds)
fit_mod_s
```

> *Question: Do the calculation similar to "Use predict() Function (Recommended). Discuss your observation.*\

> *Question: Redo the same process, but this time, obtain the predicted log-odds, odds, and probabilities when we vary the importance of `urban` while holding `religion` at its weighted mean.*

# Extra: Alternative Ways of Interpreting Results from Logit Regression

This section shows you additional options to interpret results from logit regression in addition to the options we discussed in Sections 3.3.

## Option 1: Interpret $\hat{\beta}$ Directly

From the output, we can see that $\hat{\beta}=1.17470$ is how the (expected) log-odds will change when we increase religion's degree of importance by 1. You can use the `confint()` function to extract the upper and lower bounds of the confidence intervals for $\hat{\beta}$.

```{r, echo=T, eval=T}
coef(mod_religion) # extract coefficients
```  

## Option 2: The *Divide-by-4* Rule

Third, use the *divide-by-4* rule, we can derive the largest possible change in the probability of opposing abortion.

```{r, echo=T, eval=T}
coef(mod_religion)[2]/4 # extract the "religion" coefficient 
```  
